{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599037668189",
   "display_name": "Python 3.7.4 64-bit ('DeepLearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information PLane Analysis of Deep Neural Networks via Matrix-Based Renyi's Entropy and Tensor Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix-Based Mutual Information\n",
    "## Matrix-Based Rényi's $\\alpha$-order Entropy\n",
    "\n",
    "Rényi’s $\\alpha$-order entropy is a generalization of Shannon’s entropy which is represented as:\n",
    "\n",
    "\\begin{align}\n",
    "\\ H_{\\alpha}(f) = \\frac{1}{1-\\alpha}log \\int_{X} f^{\\alpha}(x)dx\n",
    "\\end{align}\n",
    "\n",
    "Take into account that this measure is more flexible than Shanon entropy due to $\\alpha $ parameter.This equation has been really widely applied in *machine learning*, specifically $H_2$, $\\alpha = 2$, is quite interesting with Parzen windows density estimation to stimate the Probability Density Function (PDF) of a random variable. However, estimating PDF's in high-dimensional data is a challeging task and it is the typical environment in Deep Learning cases. To avoid the problem of high-dimensional PDF estimation , Giraldo et al. \\[3\\] proposed a non-parametric framework for estimating entropy directly from data using infinetely divisible kernels with similar properties as Rényi's $\\alpha$-order entropy.\n",
    "\n",
    "Giraldo's definition: Let $x_i \\in \\mathbb{X}, \\ i = 1, 2, ..., N$ denote data points and let $k : X \\times X \\rightarrow \\mathbb{R}$ be a kernel. Given the kernel matrix $\\textbf{K} \\in \\mathbb{R}^{N \\times N}$ where $(K)_ij = k(x_i, x_j)$ and the matrix $A$, $(A)_{ij} = \\frac{1}{N} \\frac{(K)_{ij}}{\\sqrt{(K)_{ii} (K)_{jj}}}$, the matrix-based **Rényi’s $\\alpha$-order entropy** is given by\n",
    "\n",
    "\\begin{align}\n",
    "S_{\\alpha}(A) = \\frac{1}{1 - \\alpha} log_{2} (tr(A^\\alpha)) = \\frac{1}{1 - \\alpha} log_{2} \\left[\\sum_{i=1}^{N} \\lambda_i (A)^{\\alpha}\\right]\n",
    "\\end{align}\n",
    "\n",
    "where tr(.) denotes the trace and $\\lambda_i (A)$ denotes the $i^{th}$ eigenvalue of A.\n",
    "\n",
    "The matrix–based Renyi’s entropy shown in the previous equation have the same functional form of the statistical quantity in a Reproducing Kernel Hilbert Space (RKHS). It means that we are **projectiing marginal distribution to an RKHS to measure entropy and mutual information** \\[1\\].\n",
    "\n",
    "The **joint entropy** between $x \\in \\mathbb{X}$ and $y \\in \\mathbb{Y}$ is defined by Giraldo et al \\[3\\] as\n",
    "\n",
    "\\begin{align}\n",
    "S_{\\alpha}(A_x, A_y) = S_{\\alpha} \\left( \\frac{A_x \\circ A_y}{tr(A_x \\circ A_y)} \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $x_i$ and $y_i$, are two different representations of the same object and $\\circ$ denotes the Hadamard product, a element-wise product. Finally, the **Mutual Information (MI)** is defined as\n",
    "\n",
    "\\begin{align}\n",
    "I_{\\alpha}(A_x, A_y) = S_{\\alpha}(A_x)  + S_{\\alpha}(A_y) - S_{\\alpha}(A_x, Ay)\n",
    "\\end{align}\n",
    "\n",
    "As we can see, this definition of MI is similar to Shannon's formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor-Based Mutual Information\n",
    "\n",
    "The output of a convolutional layer is represented as a tensor $\\mathbb{X}_i \\in \\mathbb{R}^C \\otimes \\mathbb{R}^H \\otimes \\mathbb{R}^W$ for a data point *i*, where $H$ and $W$ represents the spacial dimensional of the convolutions and $C$ the number of filters, or *channels*. Wickstrom *et al.* \\[1\\] propose to utilize tensor kernels \\[2\\] to produce a kernel matrix, $\\textbf{K} \\in \\mathbb{R}^{N \\times N}$, for the output of a convolutional layer. The ***radial basis function* (RBF) kernel** is represented as\n",
    "\n",
    "\\begin{align}\n",
    "\\ k_{ten}(X_i, X_j) = e^{-\\frac{1}{\\sigma^2} || X_i - X_j ||_{F}^{2}}\n",
    "\\end{align}\n",
    "\n",
    "where $|| . ||_{F}$ denotes the Frobenius norm and $\\sigma$ is the kernel width parameter.\n",
    "\n",
    "In practice, the tensor in previous equation is computed by reshaping the tensor into a vectorized representation, $\\mathbb{X}^{C \\times W \\times H}$, and replacing the Frobenius norm with a Euclidean norm.\n",
    "\n",
    "This tensor-based approach is posible to estimate entropy and MI in Deep Neural Networks (DNN). This estimation can be done replacing the matrix A with\n",
    "\n",
    "\\begin{align}\n",
    "\\ (A_{ten})_{ij} = \\frac{1}{N} \\frac{(K_{ten})_{ij}}{\\sqrt{(K_{ten})_{ii} (K_{ten})_{jj}}} \n",
    "\\ = \\frac{1}{N} k_{ten}(X_i, X_j)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5729, 0.3119, 0.8194, 0.4816, 0.5006, 0.6670],\n        [0.5729, 0.3119, 0.8194, 0.4816, 0.5006, 0.6670],\n        [0.5729, 0.3119, 0.8194, 0.4816, 0.5006, 0.6670],\n        [0.5729, 0.3119, 0.8194, 0.4816, 0.5006, 0.6670],\n        [0.5729, 0.3119, 0.8194, 0.4816, 0.5006, 0.6670],\n        [0.5729, 0.3119, 0.8194, 0.4816, 0.5006, 0.6670]])\ntensor([[0.5729, 0.5729, 0.5729, 0.5729, 0.5729, 0.5729],\n        [0.3119, 0.3119, 0.3119, 0.3119, 0.3119, 0.3119],\n        [0.8194, 0.8194, 0.8194, 0.8194, 0.8194, 0.8194],\n        [0.4816, 0.4816, 0.4816, 0.4816, 0.4816, 0.4816],\n        [0.5006, 0.5006, 0.5006, 0.5006, 0.5006, 0.5006],\n        [0.6670, 0.6670, 0.6670, 0.6670, 0.6670, 0.6670]])\ntensor([[0.0000, 0.2610, 0.2465, 0.0912, 0.0723, 0.0942],\n        [0.2610, 0.0000, 0.5075, 0.1698, 0.1887, 0.3552],\n        [0.2465, 0.5075, 0.0000, 0.3378, 0.3188, 0.1524],\n        [0.0912, 0.1698, 0.3378, 0.0000, 0.0190, 0.1854],\n        [0.0723, 0.1887, 0.3188, 0.0190, 0.0000, 0.1664],\n        [0.0942, 0.3552, 0.1524, 0.1854, 0.1664, 0.0000]])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.0000e+00, 1.3431e-08, 3.6708e-08, 1.7706e-03, 6.6124e-03, 1.4464e-03],\n        [1.3431e-08, 1.0000e+00, 4.9302e-16, 7.5856e-06, 2.0312e-06, 1.9427e-11],\n        [3.6708e-08, 4.9302e-16, 1.0000e+00, 6.4994e-11, 2.4272e-10, 2.5378e-05],\n        [1.7706e-03, 7.5856e-06, 6.4994e-11, 1.0000e+00, 2.6777e-01, 2.5610e-06],\n        [6.6124e-03, 2.0312e-06, 2.4272e-10, 2.6777e-01, 1.0000e+00, 9.5642e-06],\n        [1.4464e-03, 1.9427e-11, 2.5378e-05, 2.5610e-06, 9.5642e-06, 1.0000e+00]])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def RBF(x, y, sigma):\n",
    "    euclidean_norm = torch.sqrt((x[:,] - y[:,])**2)\n",
    "    print(euclidean_norm)\n",
    "    return torch.exp(-euclidean_norm/(sigma**2))\n",
    "\n",
    "rand_t = torch.rand((1,3,2,1), dtype=torch.float32)\n",
    "rand_t_flatten = torch.flatten(rand_t, 1)\n",
    "# print(torch.numel(rand_t_flatten))\n",
    "# print(rand_t_flatten.T)\n",
    "\n",
    "A = rand_t_flatten.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "print(A)\n",
    "B = rand_t_flatten.T.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "print(B)\n",
    "RBF(A, B, 0.12)\n",
    "\n",
    "# print(rand_t_flatten.expand((2,rand_t_flatten.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing the Kernel Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in *RBF Kernel*, $\\sigma$ represents the kernel width parameter and it is always critical setting. Wickstrom et al.  choose $\\sigma$ using a optimality criterion which is proposed by them \\[1\\]. A good kernel matrix should reveal the class structures present in the data. This can be accomplished by maximizing the so–called kernel alignment loss \\[4\\] between the kernel matrix of a given layer,$K_\\sigma$, and the label kernel matrix, $K_y$\n",
    "\n",
    "The kernel alignment loss is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\ A(K_a, K_b) = \\frac{\\left< K_a, K_b  \\right>_F}{||K_a||_F ||K_b||_F}\n",
    "\\end{align}\n",
    "\n",
    "where $|| . ||_F$ and $\\left< . \\right>$ denotes the Frobenius norm and inner product, respectively. Thus, they choose the optimal $\\sigma$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^* = arg \\ max_\\sigma \\ A(K_\\sigma, K_y)\n",
    "\\end{align}\n",
    "\n",
    "To stabilize the $\\sigma$ values across mini batches, we employ an exponential moving average, such that in layer $l$ at iterationt $t$ , they have\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_{l,t} = \\beta \\sigma_{l,t-1} + (1-\\beta) \\sigma_{l,t}^*\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta \\in [0,1]$ and $\\sigma_{l,1} = \\sigma_{l,1}^* $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] Wickstrøm, K., Løkse, S., Kampffmeyer, M., Yu, S., Principe, J., & Jenssen, R. (2019). Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi's Entropy and Tensor Kernels. arXiv preprint arXiv:1909.11396.\n",
    "\n",
    "\\[2\\] Marco Signoretto, Lieven De Lathauwer, and Johan AK Suykens.   A kernel-based framework to tensorial data analysis.Neural networks , 24(8):861–874, 2011\n",
    "\n",
    "\\[3\\] Luis Gonzalo Sanchez Giraldo, Murali Rao, and Jos ́e Carlos Pr ́ ıncipe.  Measures of entropy from data using infinitely divisible kernels.IEEE Transactions on Information Theory , 61:535–548, 2012\n",
    "\n",
    "\\[4\\] Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S Kandola.  On kernel-target align-ment. InAdvances in neural information processing systems , pp. 367–373, 2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}