{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi's Entropy and Tensor Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torchvision\n",
    "\n",
    "project_dir = os.path.split(os.getcwd())[0]\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from models.Paper import CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Based Mutual Information\n",
    "\n",
    "Rényi’s $\\alpha$-order entropy is a generalization of Shannon’s entropy. For a random variable $X$ with probability density function (PDF) $f(x)$ over a finite set $\\chi$, Rényi’s $\\alpha$-order entropy is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\ H_{\\alpha} f(x) = \\frac{1}{1- \\alpha} log \\int_{\\chi} f^{\\alpha}(x) dx\n",
    "\\end{align}\n",
    "\n",
    "Equation 1 has been widely applied in machine learning (Principe, 2010), and the particular case of $\\alpha= 2$, combined with Parzen window density estimation. However,accurately estimating PDFs in high-dimensional data, which is typically the case for DNNs (Deep Neural Networks), is a challenging task. To avoid the problem of high-dimensional PDF estimation, Giraldo et al. \\[3\\] proposed a non-parametric framework for estimating entropy directly from data using infinitely divisible kernels with similar properties as Rényi’s $\\alpha$-order entropy\n",
    "\n",
    "**Definition**: let $x_i \\in \\mathcal{X} , \\ i=1,2, ..., N$ denote data points and let $k : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ be an divisible positive definite kernel. Given the kernel matrix $K \\in \\mathbb{R}^{N \\times N}$ with elements $(K)_{ij} = k(x_i, x_j)$ and the matrix $A$, $(A)_{ij}= \\frac{1}{N} \\frac{(K)_{ij}}{\\sqrt{(K)_{ii} (K)_{jj}}}$, the matrix-based Rényi's $\\alpha$-order entropy is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\ S_{\\alpha} = \\frac{1}{1-\\alpha} \\log_{2} \\left( tr(A^{\\alpha})\\right) = \\frac{1}{1-\\alpha} \\left[ \\sum_{i=1}^{N} \\lambda_i (A)^\\alpha \\right]\n",
    "\\end{align}\n",
    "\n",
    "where $tr(.)$ denotes the trace and $\\lambda_i(A)$ denotes the $i^{th}$ eigenvalue of A.\n",
    "\n",
    "The matrix–based Renyi’s entropy shown in previous equation have the same functional form of the statistical quantity in a Reproducing Kernel Hilbert Space (RKHS). Essentially, It is projecting marginal distribution to an RKHS to measure entropy and mutual information.\n",
    "\n",
    "In addtion to the definition of matrix based entropy, Giraldo et al. \\[3\\] define the **joint entropy** between $x \\in \\mathcal{X}$ and $y \\in \\mathcal{Y}$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\ S_\\alpha(A_{\\mathcal{X}}, A_{\\mathcal{Y}}) =  S_\\alpha \\left( \\frac{A_{\\mathcal{X}} \\circ A_{\\mathcal{Y}}}{tr(A_{\\mathcal{X}} \\circ A_{\\mathcal{Y}}) } \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $x_i$ and $y_i$ are two different representations of the same object and $\\circ$ denotes the Hadamard product, an element-wise product. Finally, the **Mutual Information (MI)** is defined as \n",
    "\n",
    "\\begin{align}\n",
    "\\ I_\\alpha (A_{\\mathcal{X}}, A_{\\mathcal{Y}}) = S_\\alpha (A_{\\mathcal{X}}) + S_\\alpha (A_{\\mathcal{Y}}) - S_\\alpha (A_{\\mathcal{X}}, A_{\\mathcal{Y}})\n",
    "\\end{align}\n",
    "\n",
    "As we can see, this definition of MI is similar to Shannon's formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor-Based Mutual Information\n",
    "\n",
    "The output of a convolutional layer is represented as a tensor $\\mathbb{X}_i \\in \\mathbb{R}^C \\otimes \\mathbb{R}^H \\otimes \\mathbb{R}^W$ for a data point *i*, where $H$ and $W$ represents the spacial dimensional of the convolutions and $C$ the number of filters, or *channels*. Wickstrom *et al.* \\[1\\] propose to utilize tensor kernels \\[2\\] to produce a kernel matrix, $\\textbf{K} \\in \\mathbb{R}^{N \\times N}$, for the output of a convolutional layer. The ***radial basis function* (RBF) kernel** is represented as\n",
    "\n",
    "\\begin{align}\n",
    "\\ k_{ten}(X_i, X_j) = e^{-\\frac{1}{\\sigma^2} || X_i - X_j ||_{F}^{2}}\n",
    "\\end{align}\n",
    "\n",
    "where $|| . ||_{F}$ denotes the Frobenius norm and $\\sigma$ is the kernel width parameter.\n",
    "\n",
    "In practice, the tensor in previous equation is computed by reshaping the tensor into a vectorized representation, $\\mathbb{X}^{C \\times W \\times H}$, and replacing the Frobenius norm with a Euclidean norm.\n",
    "\n",
    "This tensor-based approach is posible to estimate entropy and MI in Deep Neural Networks (DNN). This estimation can be done replacing the matrix A with\n",
    "\n",
    "\\begin{align}\n",
    "\\ (A_{ten})_{ij} = \\frac{1}{N} \\frac{(K_{ten})_{ij}}{\\sqrt{(K_{ten})_{ii} (K_{ten})_{jj}}} \n",
    "\\ = \\frac{1}{N} k_{ten}(X_i, X_j)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.5925, 0.1063, 0.4847, 0.9905, 0.2020, 0.8786],\n        [0.5925, 0.1063, 0.4847, 0.9905, 0.2020, 0.8786],\n        [0.5925, 0.1063, 0.4847, 0.9905, 0.2020, 0.8786],\n        [0.5925, 0.1063, 0.4847, 0.9905, 0.2020, 0.8786],\n        [0.5925, 0.1063, 0.4847, 0.9905, 0.2020, 0.8786],\n        [0.5925, 0.1063, 0.4847, 0.9905, 0.2020, 0.8786]])\ntensor([[0.5925, 0.5925, 0.5925, 0.5925, 0.5925, 0.5925],\n        [0.1063, 0.1063, 0.1063, 0.1063, 0.1063, 0.1063],\n        [0.4847, 0.4847, 0.4847, 0.4847, 0.4847, 0.4847],\n        [0.9905, 0.9905, 0.9905, 0.9905, 0.9905, 0.9905],\n        [0.2020, 0.2020, 0.2020, 0.2020, 0.2020, 0.2020],\n        [0.8786, 0.8786, 0.8786, 0.8786, 0.8786, 0.8786]])\ntensor([[0.0000, 0.4862, 0.1078, 0.3980, 0.3905, 0.2861],\n        [0.4862, 0.0000, 0.3784, 0.8842, 0.0956, 0.7722],\n        [0.1078, 0.3784, 0.0000, 0.5058, 0.2827, 0.3939],\n        [0.3980, 0.8842, 0.5058, 0.0000, 0.7886, 0.1120],\n        [0.3905, 0.0956, 0.2827, 0.7886, 0.0000, 0.6766],\n        [0.2861, 0.7722, 0.3939, 0.1120, 0.6766, 0.0000]])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.0000e+00, 7.6818e-22, 2.0813e-05, 5.1738e-18, 1.0947e-17, 3.7732e-13],\n        [7.6818e-22, 1.0000e+00, 3.6909e-17, 3.9744e-39, 7.0170e-05, 2.8985e-34],\n        [2.0813e-05, 3.6909e-17, 1.0000e+00, 1.0768e-22, 5.2599e-13, 7.8532e-18],\n        [5.1738e-18, 3.9744e-39, 1.0768e-22, 1.0000e+00, 5.6640e-35, 1.3712e-05],\n        [1.0947e-17, 7.0170e-05, 5.2599e-13, 5.6640e-35, 1.0000e+00, 4.1307e-30],\n        [3.7732e-13, 2.8985e-34, 7.8532e-18, 1.3712e-05, 4.1307e-30, 1.0000e+00]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "def RBF(x, y, sigma):\n",
    "    euclidean_norm = torch.sqrt((x[:,] - y[:,])**2)\n",
    "    print(euclidean_norm)\n",
    "    return torch.exp(-euclidean_norm/(sigma**2))\n",
    "\n",
    "rand_t = torch.rand((1,3,2,1), dtype=torch.float32)\n",
    "rand_t_flatten = torch.flatten(rand_t, 1)\n",
    "# print(torch.numel(rand_t_flatten))\n",
    "# print(rand_t_flatten.T)\n",
    "\n",
    "A = rand_t_flatten.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "print(A)\n",
    "B = rand_t_flatten.T.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "print(B)\n",
    "RBF(A, B, 0.1)\n",
    "\n",
    "# print(rand_t_flatten.expand((2,rand_t_flatten.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing the Kernel Width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in *RBF Kernel*, $\\sigma$ represents the kernel width parameter and it is always critical setting. Wickstrom et al.  choose $\\sigma$ using a optimality criterion which is proposed by them \\[1\\]. A good kernel matrix should reveal the class structures present in the data. This can be accomplished by maximizing the so–called kernel alignment loss \\[4\\] between the kernel matrix of a given layer,$K_\\sigma$, and the label kernel matrix, $K_y$\n",
    "\n",
    "The kernel alignment loss is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\ A(K_a, K_b) = \\frac{\\left< K_a, K_b  \\right>_F}{||K_a||_F ||K_b||_F}\n",
    "\\end{align}\n",
    "\n",
    "where $|| . ||_F$ and $\\left< . \\right>$ denotes the Frobenius norm and inner product, respectively. Thus, they choose the optimal $\\sigma$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^* = arg \\ max_\\sigma \\ A(K_\\sigma, K_y)\n",
    "\\end{align}\n",
    "\n",
    "To stabilize the $\\sigma$ values across mini batches, we employ an exponential moving average, such that in layer $l$ at iterationt $t$ , they have\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_{l,t} = \\beta \\sigma_{l,t-1} + (1-\\beta) \\sigma_{l,t}^*\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta \\in [0,1]$ and $\\sigma_{l,1} = \\sigma_{l,1}^* $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelAligmentLoss(x, y):\n",
    "    return (torch.sum(x*y))/(torch.norm(x) * torch.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(182.)\ntensor(362.1768)\ntensor(0.5025)\ntensor(0.5025)\n"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4], [5,6,7,8]], dtype=torch.float32)\n",
    "b = torch.tensor([[10,20,2,1], [5, 10, 3, 2]], dtype=torch.float32)\n",
    "\n",
    "# print(a*b)\n",
    "c = torch.sum(a*b)\n",
    "print(c)\n",
    "\n",
    "d = torch.norm(a, p='fro') * torch.norm(b, p='fro')\n",
    "print(d)\n",
    "\n",
    "print(c/d)\n",
    "\n",
    "# print(a.T.shape)\n",
    "# print(a.shape)\n",
    "# torch.matmul(a, a.T)\n",
    "\n",
    "# torch.norm(a, p='fro')\n",
    "\n",
    "print(kernelAligmentLoss(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 1, 28, 28])\ntorch.Size([4, 1, 3, 3])\ntorch.Size([2, 4, 26, 26])\ntensor([[0.0776, 0.1045, 0.1015, 0.1151, 0.0810, 0.1208, 0.0765, 0.0725, 0.1094,\n         0.1409],\n        [0.1278, 0.0929, 0.0940, 0.0841, 0.1215, 0.0712, 0.1093, 0.1429, 0.0891,\n         0.0671]], grad_fn=<SoftmaxBackward>)\n"
    }
   ],
   "source": [
    "dataset = torchvision.datasets.MNIST(\"../datasets/MNIST/\", train=True, download=True)\n",
    "\n",
    "a = next(iter(dataset))\n",
    "\n",
    "\n",
    "model = CNN()\n",
    "a = torch.rand((2,1,28,28))\n",
    "first_conv = model.conv_layers[0](a)\n",
    "weight = model.conv_layers[0].weight\n",
    "print(a.shape)\n",
    "print(weight.shape)\n",
    "print(first_conv.shape)\n",
    "b = model.conv_layers(a)\n",
    "\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "print(softmax(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([13.9208, 13.8435, 14.3359, 13.7861, 14.2711])\n75\n"
    }
   ],
   "source": [
    "batch = torch.rand((10, 4, 12, 12), dtype=torch.float32)\n",
    "minibatch = batch[:5]\n",
    "minibtach_flatten = torch.flatten(minibatch, 1)\n",
    "\n",
    "distance = torch.norm(minibtach_flatten, dim=1)\n",
    "\n",
    "print(distance)\n",
    "\n",
    "print(len(torch.arange(0, 100, (100-0)/75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformation(torch.nn.Module):\n",
    "    '''\n",
    "        param step: number of steps in order to reduce the number of possible sigma \n",
    "        values.\n",
    "\n",
    "        param sigma_values: number of possible sigma values for optimizing process.\n",
    "    '''\n",
    "    def __init__(self, sigma_values=75, step=150):\n",
    "        # TODO\n",
    "        return\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # TODO\n",
    "\n",
    "        mean_distance_x = torch.tensor[torch.dist(x[i-1], x[i]) for i in range(1, len(x))].mean()\n",
    "        mean_distance_y = torch.tensor[torch.dist(y[i-1], y[i]) for i in range(1, len(y))].mean()\n",
    "        \n",
    "        return\n",
    "\n",
    "    '''\n",
    "         Tensor Based Radial Basis Function (RBF) Kernel\n",
    "    '''\n",
    "    def RBF(self, x, sigma):\n",
    "        if len(x.shape) < 2:\n",
    "            x = x.reshape(1, len(x))\n",
    "\n",
    "        a = x.expand((torch.numel(x), torch.numel(x)))\n",
    "        b = x.T.expand((torch.numel(x), torch.numel(x)))\n",
    "        euclidean_norm = torch.sqrt((a[:,] - b[:,])**2)\n",
    "        return torch.exp(-euclidean_norm/(sigma**2))\n",
    "\n",
    "    '''\n",
    "        Kernel Aligment Loss Function.\n",
    "\n",
    "        This function is used in order to obtain the optimal sigma parameter from\n",
    "        RBF kernel.  \n",
    "    '''\n",
    "    def kernelAligmentLoss(self, x, y):\n",
    "        return (torch.sum(x*y))/(torch.norm(x) * torch.norm(y))\n",
    "\n",
    "    def optimizeSigmaValue(self, x):\n",
    "        '''\n",
    "            This function is used in orter to obtain the optimal kernel width for\n",
    "            an l DNN layer\n",
    "        '''\n",
    "        # TODO\n",
    "        # note:\n",
    "        #  input kernel width : 8\n",
    "        #  label kernel width : 0.1\n",
    "        distance = tensor[torch.dist(x[i-1], x[i]) for i in range(1, len(x))].mean()\n",
    "        sigma_values = torch.arange(distance*0.1, distance*10, (distance*10 - distance*0.1)/75)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 4])\ntorch.Size([2, 4])\ntorch.Size([2, 12])\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (24) must match the existing size (12) at non-singleton dimension 1.  Target sizes: [24, 24].  Tensor sizes: [2, 12]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d9ca6b5f27eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_t_flatten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# q = RBF(rand_y[0,:], 0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# for i in range(1, len(rand_t_flatten)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d9ca6b5f27eb>\u001b[0m in \u001b[0;36mRBF\u001b[0;34m(x, sigma)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# x = x.reshape(1, len(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meuclidean_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (24) must match the existing size (12) at non-singleton dimension 1.  Target sizes: [24, 24].  Tensor sizes: [2, 12]"
     ]
    }
   ],
   "source": [
    "def RBF(x, sigma):\n",
    "    # x = x.reshape(1, len(x))\n",
    "    print(x.shape)\n",
    "    a = x.expand((torch.numel(x), torch.numel(x)))\n",
    "    b = x.T.expand((torch.numel(x), torch.numel(x)))\n",
    "    euclidean_norm = torch.sqrt((a[:,] - b[:,])**2)\n",
    "    return torch.exp(-euclidean_norm/(sigma**2))\n",
    "\n",
    "rand_t = torch.rand((2,2,6,1), dtype=torch.float32)\n",
    "rand_t_flatten = torch.flatten(rand_t, 1)\n",
    "rand_y = torch.zeros((2, 4), dtype=torch.float32)\n",
    "rand_y[0,1] = 1\n",
    "rand_y[1,3] = 1\n",
    "\n",
    "print(rand_y.shape)\n",
    "print(rand_y.shape)\n",
    "\n",
    "p = RBF(rand_t_flatten[:], 0.1)\n",
    "# q = RBF(rand_y[0,:], 0.1)\n",
    "# for i in range(1, len(rand_t_flatten)):\n",
    "#     print(\"entré\")\n",
    "#     p = torch.stack((p, RBF(rand_t_flatten[i,:], 0.1)))\n",
    "\n",
    "\n",
    "print(p.shape)\n",
    "print(q.shape)\n",
    "\n",
    "# a = torch.rand((2,2))\n",
    "# b = torch.rand((2,2))\n",
    "# c = torch.stack((a,b))\n",
    "# print(c.shape)\n",
    "\n",
    "# A = rand_t_flatten.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "# print(A)\n",
    "# B = rand_t_flatten.T.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "# print(B)\n",
    "# RBF(A, B, 0.1)\n",
    "\n",
    "# print(rand_t_flatten.expand((2,rand_t_flatten.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([144])\ntensor([[0., 1., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 0., 0., 1.],\n        [0., 0., 1., 0.]])\ntorch.Size([4, 4])\ntorch.Size([144, 144])\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (144) must match the size of tensor b (4) at non-singleton dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-1725be741a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernelAligmentLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# print(distance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-1725be741a6d>\u001b[0m in \u001b[0;36mkernelAligmentLoss\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkernelAligmentLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mkernel_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (144) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "a = torch.rand((4, 2000))\n",
    "a = torch.rand((4, 3, 12, 12))\n",
    "\n",
    "print(a[0][0].flatten().shape)\n",
    "\n",
    "# b=[]\n",
    "# for i in range(1, len(a)):\n",
    "#     b.append(torch.dist(a[i-1], a[i]))\n",
    "\n",
    "distance = torch.tensor([torch.dist(a[i-1], a[i]) for i in range(1, len(a))]).mean()\n",
    "label = torch.zeros((4, 4))\n",
    "label[0, 1] = 1\n",
    "label[1, 0] = 1\n",
    "label[2, 3] = 1\n",
    "label[3, 2] = 1\n",
    "print(label)\n",
    "\n",
    "def kernelAligmentLoss(x, y):\n",
    "    return (torch.sum(x*y))/(torch.norm(x) * torch.norm(y))\n",
    "\n",
    "kernel_label = RBF(label[0], 0.1)\n",
    "kernel_data = RBF(a[0][0].flatten(), distance-10)\n",
    "print(kernel_label.shape)\n",
    "print(kernel_data.shape)\n",
    "print(kernelAligmentLoss(kernel_data, kernel_label))\n",
    "\n",
    "# print(distance)\n",
    "# sigma_values = torch.arange(distance*0.1, distance*10, (distance*10 - distance*0.1)/75)\n",
    "# print(sigma_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] Wickstrøm, K., Løkse, S., Kampffmeyer, M., Yu, S., Principe, J., & Jenssen, R. (2019). Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi's Entropy and Tensor Kernels. arXiv preprint arXiv:1909.11396.\n",
    "\n",
    "\\[2\\] Marco Signoretto, Lieven De Lathauwer, and Johan AK Suykens.   A kernel-based framework to tensorial data analysis.Neural networks , 24(8):861–874, 2011\n",
    "\n",
    "\\[3\\] Luis Gonzalo Sanchez Giraldo, Murali Rao, and Jos ́e Carlos Pr ́ ıncipe.  Measures of entropy from data using infinitely divisible kernels.IEEE Transactions on Information Theory , 61:535–548, 2012\n",
    "\n",
    "\\[4\\] Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S Kandola.  On kernel-target align-ment. InAdvances in neural information processing systems , pp. 367–373, 2002"
   ]
  }
 ]
}