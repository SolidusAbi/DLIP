{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599058802596",
   "display_name": "Python 3.7.4 64-bit ('DeepLearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi's Entropy and Tensor Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Based Mutual Information\n",
    "\n",
    "Rényi’s $\\alpha$-order entropy is a generalization of Shannon’s entropy. For a random variable $X$ with probability density function (PDF) $f(x)$ over a finite set $\\chi$, Rényi’s $\\alpha$-order entropy is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\ H_{\\alpha} f(x) = \\frac{1}{1- \\alpha} log \\int_{\\chi} f^{\\alpha}(x) dx\n",
    "\\end{align}\n",
    "\n",
    "Equation 1 has been widely applied in machine learning (Principe, 2010), and the particular case of $\\alpha= 2$, combined with Parzen window density estimation. However,accurately estimating PDFs in high-dimensional data, which is typically the case for DNNs (Deep Neural Networks), is a challenging task. To avoid the problem of high-dimensional PDF estimation, Giraldo et al. \\[3\\] proposed a non-parametric framework for estimating entropy directly from data using infinitely divisible kernels with similar properties as Rényi’s $\\alpha$-order entropy\n",
    "\n",
    "**Definition**: let $x_i \\in \\mathcal{X} , \\ i=1,2, ..., N$ denote data points and let $k : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ be an divisible positive definite kernel. Given the kernel matrix $K \\in \\mathbb{R}^{N \\times N}$ with elements $(K)_{ij} = k(x_i, x_j)$ and the matrix $A$, $(A)_{ij}= \\frac{1}{N} \\frac{(K)_{ij}}{\\sqrt{(K)_{ii} (K)_{jj}}}$, the matrix-based Rényi's $\\alpha$-order entropy is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\ S_{\\alpha} = \\frac{1}{1-\\alpha} \\log_{2} \\left( tr(A^{\\alpha})\\right) = \\frac{1}{1-\\alpha} \\left[ \\sum_{i=1}^{N} \\lambda_i (A)^\\alpha \\right]\n",
    "\\end{align}\n",
    "\n",
    "where $tr(.)$ denotes the trace and $\\lambda_i(A)$ denotes the $i^{th}$ eigenvalue of A.\n",
    "\n",
    "The matrix–based Renyi’s entropy shown in previous equation have the same functional form of the statistical quantity in a Reproducing Kernel Hilbert Space (RKHS). Essentially, It is projecting marginal distribution to an RKHS to measure entropy and mutual information.\n",
    "\n",
    "In addtion to the definition of matrix based entropy, Giraldo et al. \\[3\\] define the **joint entropy** between $x \\in \\mathcal{X}$ and $y \\in \\mathcal{Y}$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\ S_\\alpha(A_{\\mathcal{X}}, A_{\\mathcal{Y}}) =  S_\\alpha \\left( \\frac{A_{\\mathcal{X}} \\circ A_{\\mathcal{Y}}}{tr(A_{\\mathcal{X}} \\circ A_{\\mathcal{Y}}) } \\right)\n",
    "\\end{align}\n",
    "\n",
    "where $x_i$ and $y_i$ are two different representations of the same object and $\\circ$ denotes the Hadamard product, an element-wise product. Finally, the **Mutual Information (MI)** is defined as \n",
    "\n",
    "\\begin{align}\n",
    "\\ I_\\alpha (A_{\\mathcal{X}}, A_{\\mathcal{Y}}) = S_\\alpha (A_{\\mathcal{X}}) + S_\\alpha (A_{\\mathcal{Y}}) - S_\\alpha (A_{\\mathcal{X}}, A_{\\mathcal{Y}})\n",
    "\\end{align}\n",
    "\n",
    "As we can see, this definition of MI is similar to Shannon's formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor-Based Mutual Information\n",
    "\n",
    "The output of a convolutional layer is represented as a tensor $\\mathbb{X}_i \\in \\mathbb{R}^C \\otimes \\mathbb{R}^H \\otimes \\mathbb{R}^W$ for a data point *i*, where $H$ and $W$ represents the spacial dimensional of the convolutions and $C$ the number of filters, or *channels*. Wickstrom *et al.* \\[1\\] propose to utilize tensor kernels \\[2\\] to produce a kernel matrix, $\\textbf{K} \\in \\mathbb{R}^{N \\times N}$, for the output of a convolutional layer. The ***radial basis function* (RBF) kernel** is represented as\n",
    "\n",
    "\\begin{align}\n",
    "\\ k_{ten}(X_i, X_j) = e^{-\\frac{1}{\\sigma^2} || X_i - X_j ||_{F}^{2}}\n",
    "\\end{align}\n",
    "\n",
    "where $|| . ||_{F}$ denotes the Frobenius norm and $\\sigma$ is the kernel width parameter.\n",
    "\n",
    "In practice, the tensor in previous equation is computed by reshaping the tensor into a vectorized representation, $\\mathbb{X}^{C \\times W \\times H}$, and replacing the Frobenius norm with a Euclidean norm.\n",
    "\n",
    "This tensor-based approach is posible to estimate entropy and MI in Deep Neural Networks (DNN). This estimation can be done replacing the matrix A with\n",
    "\n",
    "\\begin{align}\n",
    "\\ (A_{ten})_{ij} = \\frac{1}{N} \\frac{(K_{ten})_{ij}}{\\sqrt{(K_{ten})_{ii} (K_{ten})_{jj}}} \n",
    "\\ = \\frac{1}{N} k_{ten}(X_i, X_j)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RBF(x, y, sigma):\n",
    "    euclidean_norm = torch.sqrt((x[:,] - y[:,])**2)\n",
    "    print(euclidean_norm)\n",
    "    return torch.exp(-euclidean_norm/(sigma**2))\n",
    "\n",
    "rand_t = torch.rand((1,3,2,1), dtype=torch.float32)\n",
    "rand_t_flatten = torch.flatten(rand_t, 1)\n",
    "# print(torch.numel(rand_t_flatten))\n",
    "# print(rand_t_flatten.T)\n",
    "\n",
    "A = rand_t_flatten.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "print(A)\n",
    "B = rand_t_flatten.T.expand((torch.numel(rand_t_flatten), torch.numel(rand_t_flatten)))\n",
    "print(B)\n",
    "RBF(A, B, 0.12)\n",
    "\n",
    "# print(rand_t_flatten.expand((2,rand_t_flatten.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing the Kernel Width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in *RBF Kernel*, $\\sigma$ represents the kernel width parameter and it is always critical setting. Wickstrom et al.  choose $\\sigma$ using a optimality criterion which is proposed by them \\[1\\]. A good kernel matrix should reveal the class structures present in the data. This can be accomplished by maximizing the so–called kernel alignment loss \\[4\\] between the kernel matrix of a given layer,$K_\\sigma$, and the label kernel matrix, $K_y$\n",
    "\n",
    "The kernel alignment loss is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\ A(K_a, K_b) = \\frac{\\left< K_a, K_b  \\right>_F}{||K_a||_F ||K_b||_F}\n",
    "\\end{align}\n",
    "\n",
    "where $|| . ||_F$ and $\\left< . \\right>$ denotes the Frobenius norm and inner product, respectively. Thus, they choose the optimal $\\sigma$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^* = arg \\ max_\\sigma \\ A(K_\\sigma, K_y)\n",
    "\\end{align}\n",
    "\n",
    "To stabilize the $\\sigma$ values across mini batches, we employ an exponential moving average, such that in layer $l$ at iterationt $t$ , they have\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_{l,t} = \\beta \\sigma_{l,t-1} + (1-\\beta) \\sigma_{l,t}^*\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta \\in [0,1]$ and $\\sigma_{l,1} = \\sigma_{l,1}^* $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d5a8d96f425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] Wickstrøm, K., Løkse, S., Kampffmeyer, M., Yu, S., Principe, J., & Jenssen, R. (2019). Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi's Entropy and Tensor Kernels. arXiv preprint arXiv:1909.11396.\n",
    "\n",
    "\\[2\\] Marco Signoretto, Lieven De Lathauwer, and Johan AK Suykens.   A kernel-based framework to tensorial data analysis.Neural networks , 24(8):861–874, 2011\n",
    "\n",
    "\\[3\\] Luis Gonzalo Sanchez Giraldo, Murali Rao, and Jos ́e Carlos Pr ́ ıncipe.  Measures of entropy from data using infinitely divisible kernels.IEEE Transactions on Information Theory , 61:535–548, 2012\n",
    "\n",
    "\\[4\\] Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S Kandola.  On kernel-target align-ment. InAdvances in neural information processing systems , pp. 367–373, 2002"
   ]
  }
 ]
}